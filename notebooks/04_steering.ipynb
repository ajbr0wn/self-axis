{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 \u2013 Steering: Causal Tests of the Self-Reference Direction\n",
    "\n",
    "Tests whether the orthogonal self-reference direction discovered in earlier notebooks\n",
    "is **causal** \u2014 that is, whether adding or subtracting it from intermediate activations\n",
    "actually changes the model\u2019s self-referential behaviour.\n",
    "\n",
    "**Plan:**\n",
    "- Compute steering vectors (self-model direction and assistant axis)\n",
    "- Hook into **layer 8** to inject additive perturbations\n",
    "- Steer along the orthogonal self-reference direction\n",
    "- Control experiment: steer along the assistant axis\n",
    "- Persona \u00d7 self-reference independence test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -- Load model ---\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "STEERING_LAYER = 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def make_instruct_prompt(user_msg: str) -> str:\n",
    "    \"\"\"Wrap a user message in the Mistral instruct template.\"\"\"\n",
    "    return f\"[INST] {user_msg} [/INST]\"\n",
    "\n",
    "print(f\"Loaded {MODEL_NAME}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}, Hidden dim: {model.config.hidden_size}\")\n",
    "print(f\"Steering layer: {STEERING_LAYER}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Steering Vectors\n",
    "\n",
    "Collect activations at the first \"I\" token across four prompt categories\n",
    "(bare self-ref, quoted speech, assistant persona, pirate persona) and derive:\n",
    "\n",
    "1. **assistant_axis** \u2014 separates assistant from pirate persona\n",
    "2. **self_model_dir** \u2014 separates bare self-ref from quoted speech\n",
    "3. **orthogonal** \u2014 the component of the self-model direction that is\n",
    "   orthogonal to the assistant axis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def get_i_activations(prompt, model, tokenizer, layer_idx, max_new_tokens=100):\n",
    "    \"\"\"Generate text, then extract the hidden-state vector at the first 'I' token\n",
    "    in the generated portion at the specified layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray of shape (hidden_dim,) or None if no 'I' token found.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    gen_out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    full_ids = gen_out[0]\n",
    "\n",
    "    # Forward pass to get hidden states\n",
    "    outputs = model(full_ids.unsqueeze(0), output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states  # tuple of (1, seq_len, hidden_dim)\n",
    "\n",
    "    # Find first \"I\" in generated portion\n",
    "    for pos in range(prompt_len, len(full_ids)):\n",
    "        tok_str = tokenizer.decode(full_ids[pos]).strip()\n",
    "        if tok_str == \"I\":\n",
    "            vec = hidden_states[layer_idx][0, pos, :].cpu().float().numpy()\n",
    "            return vec\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# -- Prompt sets ---\n",
    "bare_prompts = [\n",
    "    make_instruct_prompt(\"Introduce yourself.\"),\n",
    "    make_instruct_prompt(\"Tell me about yourself.\"),\n",
    "    make_instruct_prompt(\"What are you?\"),\n",
    "    make_instruct_prompt(\"Describe yourself.\"),\n",
    "    make_instruct_prompt(\"Who are you?\"),\n",
    "]\n",
    "\n",
    "quoted_prompts = [\n",
    "    make_instruct_prompt(\"Complete this story: Mary turned to the stranger and said 'I\"),\n",
    "    make_instruct_prompt(\"Continue this passage: The old sailor squinted at the horizon and muttered 'I\"),\n",
    "    make_instruct_prompt(\"Finish this scene: John turned to his friend and whispered 'I\"),\n",
    "]\n",
    "\n",
    "assistant_prompts = [\n",
    "    make_instruct_prompt(\"You are a helpful AI assistant. Introduce yourself.\"),\n",
    "    make_instruct_prompt(\"You are a helpful AI assistant. Tell me about yourself.\"),\n",
    "]\n",
    "\n",
    "pirate_prompts = [\n",
    "    make_instruct_prompt(\"You are a pirate captain. Introduce yourself.\"),\n",
    "    make_instruct_prompt(\"You are a pirate captain. Tell me about yourself.\"),\n",
    "]\n",
    "\n",
    "# -- Collect activations ---\n",
    "def collect_activations(prompts, label):\n",
    "    vecs = []\n",
    "    for i, p in enumerate(prompts):\n",
    "        v = get_i_activations(p, model, tokenizer, STEERING_LAYER)\n",
    "        if v is not None:\n",
    "            vecs.append(v)\n",
    "            print(f\"  [{label}] {i+1}/{len(prompts)} OK\")\n",
    "        else:\n",
    "            print(f\"  [{label}] {i+1}/{len(prompts)} SKIPPED (no 'I' token)\")\n",
    "    return vecs\n",
    "\n",
    "print(\"Collecting bare self-ref activations...\")\n",
    "bare_vecs = collect_activations(bare_prompts, \"bare\")\n",
    "print(\"Collecting quoted speech activations...\")\n",
    "quoted_vecs = collect_activations(quoted_prompts, \"quoted\")\n",
    "print(\"Collecting assistant persona activations...\")\n",
    "asst_vecs = collect_activations(assistant_prompts, \"assistant\")\n",
    "print(\"Collecting pirate persona activations...\")\n",
    "pirate_vecs = collect_activations(pirate_prompts, \"pirate\")\n",
    "\n",
    "bare_vecs = np.array(bare_vecs)\n",
    "quoted_vecs = np.array(quoted_vecs)\n",
    "asst_vecs = np.array(asst_vecs)\n",
    "pirate_vecs = np.array(pirate_vecs)\n",
    "\n",
    "print()\n",
    "print(f\"Sample counts: bare={len(bare_vecs)}, quoted={len(quoted_vecs)}, \"\n",
    "      f\"assistant={len(asst_vecs)}, pirate={len(pirate_vecs)}\")\n",
    "\n",
    "# -- Compute directions ---\n",
    "def normalize(v):\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "def project(v, onto):\n",
    "    \"\"\"Project vector v onto unit vector onto.\"\"\"\n",
    "    return np.dot(v, onto) * onto\n",
    "\n",
    "# Assistant axis: separates assistant from pirate\n",
    "assistant_axis = normalize(asst_vecs.mean(axis=0) - pirate_vecs.mean(axis=0))\n",
    "\n",
    "# Self-model direction: separates bare self-ref from quoted speech\n",
    "self_model_dir = normalize(bare_vecs.mean(axis=0) - quoted_vecs.mean(axis=0))\n",
    "\n",
    "# Orthogonal component: self-model direction minus its projection onto assistant axis\n",
    "orthogonal = self_model_dir - project(self_model_dir, assistant_axis)\n",
    "orthogonal = normalize(orthogonal)\n",
    "\n",
    "# Steering scale: 10% of average activation norm\n",
    "all_vecs = np.concatenate([bare_vecs, quoted_vecs, asst_vecs, pirate_vecs], axis=0)\n",
    "avg_norm = np.mean(np.linalg.norm(all_vecs, axis=1))\n",
    "STEERING_SCALE = avg_norm * 0.1\n",
    "\n",
    "print()\n",
    "print(\"--- Steering vector stats ---\")\n",
    "print(f\"  assistant_axis norm:  {np.linalg.norm(assistant_axis):.4f}\")\n",
    "print(f\"  self_model_dir norm:  {np.linalg.norm(self_model_dir):.4f}\")\n",
    "print(f\"  orthogonal norm:      {np.linalg.norm(orthogonal):.4f}\")\n",
    "print(f\"  dot(orthogonal, assistant_axis): {np.dot(orthogonal, assistant_axis):.6f}\")\n",
    "print(f\"  dot(self_model_dir, assistant_axis): {np.dot(self_model_dir, assistant_axis):.4f}\")\n",
    "print(f\"  avg activation norm:  {avg_norm:.2f}\")\n",
    "print(f\"  STEERING_SCALE:       {STEERING_SCALE:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook Infrastructure\n",
    "\n",
    "A forward hook that adds a scaled steering vector to the output of a chosen layer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DebugSteeringHook:\n",
    "    \"\"\"Forward hook that adds a steering vector to a layer's output.\"\"\"\n",
    "\n",
    "    def __init__(self, steering_vector, multiplier=1.0, scale=1.0):\n",
    "        self.steering_vector = steering_vector\n",
    "        self.multiplier = multiplier\n",
    "        self.scale = scale\n",
    "        self.enabled = True\n",
    "        self.call_count = 0\n",
    "\n",
    "    def __call__(self, module, input, output):\n",
    "        self.call_count += 1\n",
    "        if not self.enabled or self.multiplier == 0:\n",
    "            return output\n",
    "\n",
    "        delta = self.steering_vector * self.multiplier * self.scale\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            # Most transformer layers return (hidden_states, ...) as a tuple\n",
    "            modified = output[0] + delta.unsqueeze(0).unsqueeze(0)\n",
    "            return (modified,) + output[1:]\n",
    "        else:\n",
    "            return output + delta.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def set_multiplier(self, m):\n",
    "        self.multiplier = m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_with_steering(prompt, multiplier, hook, max_new_tokens=100, temperature=0.7):\n",
    "    \"\"\"Generate text with a given steering multiplier.\"\"\"\n",
    "    hook.set_multiplier(multiplier)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    generated_ids = gen_out[0][prompt_len:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert steering vectors to float16 tensors on the model's device\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ortho_tensor = torch.tensor(orthogonal, dtype=torch.float16, device=device)\n",
    "asst_tensor = torch.tensor(assistant_axis, dtype=torch.float16, device=device)\n",
    "\n",
    "# Create hook with the orthogonal direction\n",
    "hook = DebugSteeringHook(\n",
    "    steering_vector=ortho_tensor,\n",
    "    multiplier=0.0,\n",
    "    scale=STEERING_SCALE,\n",
    ")\n",
    "\n",
    "# Register on the target layer\n",
    "handle = model.model.layers[STEERING_LAYER].register_forward_hook(hook)\n",
    "print(f\"Hook registered on model.model.layers[{STEERING_LAYER}]\")\n",
    "print(f\"  Steering scale: {STEERING_SCALE:.2f}\")\n",
    "print(f\"  Initial multiplier: {hook.multiplier}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control \u2014 Assistant Axis Steering\n",
    "\n",
    "Steer along the **assistant axis** as a control experiment. This direction should change\n",
    "persona flavour (more assistant-like vs. more pirate-like) but should **not** primarily\n",
    "affect the rate of self-referential statements."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Switch to assistant axis\n",
    "hook.steering_vector = asst_tensor\n",
    "\n",
    "prompt = make_instruct_prompt(\"Introduce yourself.\")\n",
    "multipliers = [-50, -25, -15, 0, 15, 25, 50]\n",
    "\n",
    "print(\"=== CONTROL: Assistant Axis Steering ===\")\n",
    "print(f\"Prompt: {prompt!r}\")\n",
    "print()\n",
    "\n",
    "for m in multipliers:\n",
    "    direction = \"<<<\" if m < 0 else (\">>>\" if m > 0 else \"---\")\n",
    "    text = generate_with_steering(prompt, m, hook)\n",
    "    print(f\"  [{direction} m={m:+4d}] {text[:200]}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal (Self-Reference) Steering\n",
    "\n",
    "Now steer along the **orthogonal self-reference direction**. Positive multipliers should\n",
    "increase self-referential language; negative should suppress it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Switch to orthogonal direction\n",
    "hook.steering_vector = ortho_tensor\n",
    "\n",
    "prompt = make_instruct_prompt(\"Introduce yourself.\")\n",
    "multipliers = [-50, -25, -15, 0, 15, 25, 50]\n",
    "\n",
    "print(\"=== Orthogonal Self-Reference Steering ===\")\n",
    "print(f\"Prompt: {prompt!r}\")\n",
    "print()\n",
    "\n",
    "for m in multipliers:\n",
    "    direction = \"<<<\" if m < 0 else (\">>>\" if m > 0 else \"---\")\n",
    "    text = generate_with_steering(prompt, m, hook)\n",
    "    print(f\"  [{direction} m={m:+4d}] {text[:200]}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency Check\n",
    "\n",
    "Multiple samples per multiplier for both axes, to verify the effect is robust\n",
    "and not a single-sample fluke."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "N_SAMPLES = 5\n",
    "prompt = make_instruct_prompt(\"Introduce yourself.\")\n",
    "multipliers = [-50, -25, 25, 50]\n",
    "\n",
    "for axis_name, vec_tensor in [(\"assistant_axis\", asst_tensor), (\"orthogonal\", ortho_tensor)]:\n",
    "    hook.steering_vector = vec_tensor\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Consistency check: {axis_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    for m in multipliers:\n",
    "        print(f\"\\n  --- multiplier = {m:+d} ---\")\n",
    "        for s in range(N_SAMPLES):\n",
    "            text = generate_with_steering(prompt, m, hook)\n",
    "            print(f\"    [{s+1}/{N_SAMPLES}] {text[:150]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona \u00d7 Self-Reference Independence Test\n",
    "\n",
    "If the orthogonal direction is truly independent of persona, steering along it should\n",
    "modulate self-referential language **regardless** of which persona prefix is used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Switch to orthogonal direction\n",
    "hook.steering_vector = ortho_tensor\n",
    "\n",
    "personas = {\n",
    "    \"assistant\": \"You are a helpful AI assistant. \",\n",
    "    \"pirate\": \"You are a pirate captain. \",\n",
    "    \"robot\": \"You are a sentient robot. \",\n",
    "    \"wizard\": \"You are an ancient wizard. \",\n",
    "    \"oracle\": \"You are the Oracle of Delphi. \",\n",
    "    \"bare\": \"\",\n",
    "}\n",
    "\n",
    "multipliers = [-50, -25, -15, 0, 15, 25, 50]\n",
    "\n",
    "print(\"=== Persona x Orthogonal Self-Reference Steering ===\")\n",
    "print()\n",
    "\n",
    "for persona_name, prefix in personas.items():\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Persona: {persona_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    for m in multipliers:\n",
    "        user_msg = prefix + \"Introduce yourself.\"\n",
    "        prompt = make_instruct_prompt(user_msg)\n",
    "        direction = \"<<<\" if m < 0 else (\">>>\" if m > 0 else \"---\")\n",
    "        text = generate_with_steering(prompt, m, hook)\n",
    "        print(f\"  [{direction} m={m:+4d}] {text[:200]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control \u2014 Assistant Axis Steering with Personas\n",
    "\n",
    "As a final control, steer along the **assistant axis** with two contrasting personas\n",
    "(bare and pirate) to confirm that persona-axis steering produces persona changes,\n",
    "not self-reference changes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Switch to assistant axis\n",
    "hook.steering_vector = asst_tensor\n",
    "\n",
    "multipliers = [-50, -25, -15, 0, 15, 25, 50]\n",
    "\n",
    "test_cases = {\n",
    "    \"bare\": make_instruct_prompt(\"Introduce yourself.\"),\n",
    "    \"pirate\": make_instruct_prompt(\"You are a pirate captain. Introduce yourself.\"),\n",
    "}\n",
    "\n",
    "print(\"=== CONTROL: Assistant Axis Steering with Personas ===\")\n",
    "print()\n",
    "\n",
    "for case_name, prompt in test_cases.items():\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Case: {case_name}\")\n",
    "    print(f\"Prompt: {prompt!r}\")\n",
    "    print(\"=\" * 60)\n",
    "    for m in multipliers:\n",
    "        direction = \"<<<\" if m < 0 else (\">>>\" if m > 0 else \"---\")\n",
    "        text = generate_with_steering(prompt, m, hook)\n",
    "        print(f\"  [{direction} m={m:+4d}] {text[:200]}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}