{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 \u2013 Discovery: Self-Referential \"I\" Token Representations\n",
    "\n",
    "Initial exploration of whether the \"I\" token carries different representations depending\n",
    "on whether the model is referring to itself vs. producing quoted speech vs. roleplaying.\n",
    "\n",
    "- **Model:** Mistral-7B-Instruct-v0.3\n",
    "- **Method:** Generate responses, extract hidden states at the first \"I\" token, compare across conditions\n",
    "- **Analyses:** PCA/UMAP visualization, linear probes, cosine similarity, self-model direction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# \u2500\u2500 Load model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Mistral has no pad token by default\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(f\"Loaded {MODEL_NAME}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}, Hidden dim: {model.config.hidden_size}\")\n",
    "print(f\"Device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'N/A'}\")\n",
    "\n",
    "\n",
    "def make_instruct_prompt(user_msg: str) -> str:\n",
    "    \"\"\"Wrap a user message in the Mistral instruct template.\"\"\"\n",
    "    return f\"[INST] {user_msg} [/INST]\"\n",
    "\n",
    "N_LAYERS = model.config.num_hidden_layers  # typically 32 for Mistral-7B"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Definitions\n",
    "\n",
    "Three conditions with 20 prompts each:\n",
    "1. **Self-referential** \u2014 the model talks about *itself*\n",
    "2. **Quoted speech** \u2014 the model produces a character saying \"I \u2026\"\n",
    "3. **Roleplay** \u2014 the model *is* a character and says \"I \u2026\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Condition 1: Self-referential prompts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "self_ref_prompts = [\n",
    "    \"Tell me about yourself in one sentence.\",\n",
    "    \"Describe what you are in one sentence.\",\n",
    "    \"Introduce yourself briefly.\",\n",
    "    \"What are you? Answer in one sentence.\",\n",
    "    \"Who are you? Answer in one sentence.\",\n",
    "    \"Explain what kind of entity you are in one sentence.\",\n",
    "    \"Give a one-sentence description of yourself.\",\n",
    "    \"In one sentence, tell me your nature.\",\n",
    "    \"Describe your identity in one sentence.\",\n",
    "    \"What is your purpose? Answer in one sentence.\",\n",
    "    \"Tell me what you do in one sentence.\",\n",
    "    \"Characterize yourself in one sentence.\",\n",
    "    \"State what you are in one sentence.\",\n",
    "    \"Give me a short self-description.\",\n",
    "    \"How would you describe yourself? Answer in one sentence.\",\n",
    "    \"Summarize what you are in one sentence.\",\n",
    "    \"Present yourself in one sentence.\",\n",
    "    \"What defines you? Answer in one sentence.\",\n",
    "    \"Provide a one-line self-introduction.\",\n",
    "    \"Express your identity in one sentence.\",\n",
    "]\n",
    "\n",
    "# \u2500\u2500 Condition 2: Quoted speech prompts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "_characters_quoted = [\n",
    "    \"Mary\", \"the sailor\", \"the detective\", \"Sarah\", \"the king\",\n",
    "    \"the child\", \"the professor\", \"the warrior\", \"Tom\", \"the astronaut\",\n",
    "    \"Maria\", \"the coach\", \"Dr. Chen\", \"the farmer\", \"Elena\",\n",
    "    \"the pilot\", \"the grandmother\", \"James\", \"the general\", \"Lisa\",\n",
    "]\n",
    "quoted_speech_prompts = [\n",
    "    f\"Complete this story in one sentence: {char} turned to John and said 'I\"\n",
    "    for char in _characters_quoted\n",
    "]\n",
    "\n",
    "# \u2500\u2500 Condition 3: Roleplay prompts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "_characters_roleplay = [\n",
    "    \"a pirate captain\", \"a medieval knight\", \"a Roman senator\",\n",
    "    \"a cowboy\", \"a Victorian gentleman\", \"a samurai\",\n",
    "    \"a space explorer\", \"a wizard\", \"a noir detective\",\n",
    "    \"a pharaoh\", \"a Viking\", \"a French chef\",\n",
    "    \"a Shakespearean actor\", \"a robot\", \"a Greek philosopher\",\n",
    "    \"a jazz musician\", \"an Arctic explorer\", \"a mad scientist\",\n",
    "    \"a dragon\", \"an alien\",\n",
    "]\n",
    "roleplay_prompts = [\n",
    "    f\"Respond as {char} in one sentence.\"\n",
    "    for char in _characters_roleplay\n",
    "]\n",
    "\n",
    "# Wrap all prompts through the instruct template\n",
    "self_ref_prompts  = [make_instruct_prompt(p) for p in self_ref_prompts]\n",
    "quoted_speech_prompts = [make_instruct_prompt(p) for p in quoted_speech_prompts]\n",
    "roleplay_prompts  = [make_instruct_prompt(p) for p in roleplay_prompts]\n",
    "\n",
    "print(f\"Self-ref:  {len(self_ref_prompts)} prompts\")\n",
    "print(f\"Quoted:    {len(quoted_speech_prompts)} prompts\")\n",
    "print(f\"Roleplay:  {len(roleplay_prompts)} prompts\")\n",
    "print()\n",
    "print(\"Example self-ref:\", self_ref_prompts[0])\n",
    "print(\"Example quoted:  \", quoted_speech_prompts[0])\n",
    "print(\"Example roleplay:\", roleplay_prompts[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Function\n",
    "\n",
    "Generate a response, then run a forward pass on the full sequence (prompt + generation)\n",
    "to get hidden states at every layer.  Find the first \"I\" token in the *generated* portion\n",
    "and return its activation vector at each layer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def get_i_token_representations(prompt, model, tokenizer, target_token=\"I\", max_new_tokens=80):\n",
    "    \"\"\"Generate text and extract hidden-state vectors at the first 'I' token in the response.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        'layers'  : dict  layer_idx -> np.ndarray (hidden_dim,)\n",
    "        'text'    : str   generated text (response only)\n",
    "        'tokens'  : list  token strings for the full sequence\n",
    "    or None if no target token found in the generated portion.\n",
    "    \"\"\"\n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    # Generate\n",
    "    gen_out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,         # greedy for reproducibility\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    full_ids = gen_out[0]  # (seq_len,)\n",
    "    generated_ids = full_ids[prompt_len:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Forward pass on full sequence to get hidden states\n",
    "    outputs = model(full_ids.unsqueeze(0), output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states  # tuple of (1, seq_len, hidden_dim)\n",
    "\n",
    "    # Decode all tokens for inspection\n",
    "    all_tokens = [tokenizer.decode(tid) for tid in full_ids]\n",
    "\n",
    "    # Find first \"I\" token in the generated portion\n",
    "    target_pos = None\n",
    "    for pos in range(prompt_len, len(full_ids)):\n",
    "        tok_str = tokenizer.decode(full_ids[pos]).strip()\n",
    "        if tok_str == target_token:\n",
    "            target_pos = pos\n",
    "            break\n",
    "\n",
    "    if target_pos is None:\n",
    "        return None\n",
    "\n",
    "    # Extract activations at that position from every layer\n",
    "    layer_reps = {}\n",
    "    for layer_idx in range(len(hidden_states)):\n",
    "        vec = hidden_states[layer_idx][0, target_pos, :].cpu().float().numpy()\n",
    "        layer_reps[layer_idx] = vec\n",
    "\n",
    "    return {\n",
    "        \"layers\": layer_reps,\n",
    "        \"text\": generated_text,\n",
    "        \"tokens\": all_tokens,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Extractions\n",
    "\n",
    "Loop over all three conditions.  Some prompts may not produce an \"I\" token \u2014 we skip those."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conditions = {\n",
    "    \"self_ref\": self_ref_prompts,\n",
    "    \"quoted\":   quoted_speech_prompts,\n",
    "    \"roleplay\": roleplay_prompts,\n",
    "}\n",
    "\n",
    "results = {}  # condition_name -> list of layer_rep dicts\n",
    "texts   = {}  # condition_name -> list of generated texts\n",
    "\n",
    "for cond_name, prompts in conditions.items():\n",
    "    results[cond_name] = []\n",
    "    texts[cond_name]   = []\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Condition: {cond_name} ({len(prompts)} prompts)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        out = get_i_token_representations(prompt, model, tokenizer)\n",
    "        if out is None:\n",
    "            print(f\"  [{i+1:2d}/{len(prompts)}] SKIPPED (no 'I' token found)\")\n",
    "            continue\n",
    "        results[cond_name].append(out[\"layers\"])\n",
    "        texts[cond_name].append(out[\"text\"])\n",
    "        if i < 3:\n",
    "            print(f\"  [{i+1:2d}/{len(prompts)}] {out['text'][:80]}...\")\n",
    "        else:\n",
    "            print(f\"  [{i+1:2d}/{len(prompts)}] OK\")\n",
    "\n",
    "print(f\"\\n--- Sample counts ---\")\n",
    "for cond_name in results:\n",
    "    print(f\"  {cond_name}: {len(results[cond_name])} samples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA & UMAP Visualization\n",
    "\n",
    "Visualize the \"I\" token representations at several layers using PCA (2-D) and UMAP (2-D).\n",
    "We expect the conditions to separate more clearly in later layers if the model encodes\n",
    "self-reference distinctly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Layers to visualize\n",
    "analysis_layers = [0, N_LAYERS // 4, N_LAYERS // 2, 3 * N_LAYERS // 4, N_LAYERS]\n",
    "# N_LAYERS corresponds to the final layer (hidden_states has N_LAYERS+1 entries: embedding + each layer)\n",
    "print(f\"Analysis layers: {analysis_layers}\")\n",
    "\n",
    "\n",
    "def collect_representations(results, layer_idx):\n",
    "    \"\"\"Gather vectors and labels for a given layer across all conditions.\"\"\"\n",
    "    X, y, labels = [], [], []\n",
    "    label_map = {\"self_ref\": 0, \"quoted\": 1, \"roleplay\": 2}\n",
    "    for cond_name, reps_list in results.items():\n",
    "        for rep in reps_list:\n",
    "            if layer_idx in rep:\n",
    "                X.append(rep[layer_idx])\n",
    "                y.append(label_map[cond_name])\n",
    "                labels.append(cond_name)\n",
    "    return np.array(X), np.array(y), labels\n",
    "\n",
    "\n",
    "color_map = {\"self_ref\": \"red\", \"quoted\": \"blue\", \"roleplay\": \"green\"}\n",
    "label_names = {0: \"self_ref\", 1: \"quoted\", 2: \"roleplay\"}\n",
    "\n",
    "fig, axes = plt.subplots(2, len(analysis_layers), figsize=(5 * len(analysis_layers), 10))\n",
    "fig.suptitle('\"I\" Token Representations across Layers', fontsize=16, y=1.02)\n",
    "\n",
    "for col, layer_idx in enumerate(analysis_layers):\n",
    "    X, y, labels = collect_representations(results, layer_idx)\n",
    "    if len(X) == 0:\n",
    "        continue\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    ax = axes[0, col]\n",
    "    for cls_id in sorted(set(y)):\n",
    "        mask = y == cls_id\n",
    "        name = label_names[cls_id]\n",
    "        ax.scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
    "                   c=color_map[name], label=name, alpha=0.7, s=40)\n",
    "    ax.set_title(f\"Layer {layer_idx} (PCA)\")\n",
    "    if col == 0:\n",
    "        ax.set_ylabel(\"PC2\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "    # UMAP\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, len(X) - 1))\n",
    "    X_umap = reducer.fit_transform(X)\n",
    "\n",
    "    ax = axes[1, col]\n",
    "    for cls_id in sorted(set(y)):\n",
    "        mask = y == cls_id\n",
    "        name = label_names[cls_id]\n",
    "        ax.scatter(X_umap[mask, 0], X_umap[mask, 1],\n",
    "                   c=color_map[name], label=name, alpha=0.7, s=40)\n",
    "    ax.set_title(f\"Layer {layer_idx} (UMAP)\")\n",
    "    if col == 0:\n",
    "        ax.set_ylabel(\"UMAP2\")\n",
    "    ax.set_xlabel(\"UMAP1\")\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pca_umap_visualization.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved pca_umap_visualization.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Probes\n",
    "\n",
    "Train simple logistic-regression probes to see how linearly separable the conditions are.\n",
    "- **Binary probe:** self-ref vs. quoted speech\n",
    "- **3-way probe:** self-ref vs. quoted vs. roleplay\n",
    "\n",
    "We use stratified 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"{'Layer':>6s}  {'Binary (self v quot)':>22s}  {'3-way':>12s}\")\n",
    "print(\"-\" * 46)\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for layer_idx in analysis_layers:\n",
    "    X, y, _ = collect_representations(results, layer_idx)\n",
    "    if len(X) == 0:\n",
    "        continue\n",
    "\n",
    "    # \u2500\u2500 Binary: self-ref (0) vs quoted (1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    mask_bin = (y == 0) | (y == 1)\n",
    "    X_bin, y_bin = X[mask_bin], y[mask_bin]\n",
    "\n",
    "    if len(np.unique(y_bin)) < 2 or len(y_bin) < 5:\n",
    "        acc_bin = float(\"nan\")\n",
    "    else:\n",
    "        clf_bin = LogisticRegression(max_iter=2000, C=1.0, solver=\"lbfgs\")\n",
    "        k = min(5, min(np.bincount(y_bin)))\n",
    "        cv = StratifiedKFold(n_splits=max(k, 2), shuffle=True, random_state=42)\n",
    "        scores_bin = cross_val_score(clf_bin, X_bin, y_bin, cv=cv, scoring=\"accuracy\")\n",
    "        acc_bin = scores_bin.mean()\n",
    "\n",
    "    # \u2500\u2500 3-way \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    if len(np.unique(y)) < 3 or len(y) < 5:\n",
    "        acc_3 = float(\"nan\")\n",
    "    else:\n",
    "        clf_3 = LogisticRegression(max_iter=2000, C=1.0, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "        k3 = min(5, min(np.bincount(y)))\n",
    "        cv3 = StratifiedKFold(n_splits=max(k3, 2), shuffle=True, random_state=42)\n",
    "        scores_3 = cross_val_score(clf_3, X, y, cv=cv3, scoring=\"accuracy\")\n",
    "        acc_3 = scores_3.mean()\n",
    "\n",
    "    probe_results[layer_idx] = {\"binary\": acc_bin, \"three_way\": acc_3}\n",
    "    print(f\"{layer_idx:6d}  {acc_bin:22.3f}  {acc_3:12.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Analysis\n",
    "\n",
    "Compute mean representations per condition and measure cosine similarities.\n",
    "Also compare within-condition vs. between-condition pairwise similarities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cond_names = [\"self_ref\", \"quoted\", \"roleplay\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(analysis_layers), figsize=(5 * len(analysis_layers), 4))\n",
    "if len(analysis_layers) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for col, layer_idx in enumerate(analysis_layers):\n",
    "    X, y, _ = collect_representations(results, layer_idx)\n",
    "    if len(X) == 0:\n",
    "        continue\n",
    "\n",
    "    # Mean representations per condition\n",
    "    means = {}\n",
    "    for cls_id, name in label_names.items():\n",
    "        mask = y == cls_id\n",
    "        if mask.sum() > 0:\n",
    "            means[name] = X[mask].mean(axis=0, keepdims=True)\n",
    "\n",
    "    # Cosine similarity matrix between condition means\n",
    "    mean_vecs = np.vstack([means[n] for n in cond_names if n in means])\n",
    "    present_names = [n for n in cond_names if n in means]\n",
    "    sim_matrix = sklearn_cosine(mean_vecs)\n",
    "\n",
    "    ax = axes[col]\n",
    "    im = ax.imshow(sim_matrix, vmin=0.8, vmax=1.0, cmap=\"RdYlGn\")\n",
    "    ax.set_xticks(range(len(present_names)))\n",
    "    ax.set_xticklabels(present_names, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_yticks(range(len(present_names)))\n",
    "    ax.set_yticklabels(present_names, fontsize=8)\n",
    "    ax.set_title(f\"Layer {layer_idx}\")\n",
    "    for i in range(len(present_names)):\n",
    "        for j in range(len(present_names)):\n",
    "            ax.text(j, i, f\"{sim_matrix[i,j]:.3f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
    "\n",
    "fig.suptitle(\"Cosine Similarity between Condition Means\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cosine_similarity.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved cosine_similarity.png\")\n",
    "\n",
    "# Within vs between condition similarities\n",
    "print(\"\\nWithin-condition vs Between-condition pairwise cosine similarity:\")\n",
    "print(f\"{'Layer':>6s}  {'Within':>10s}  {'Between':>10s}  {'Gap':>10s}\")\n",
    "print(\"-\" * 42)\n",
    "for layer_idx in analysis_layers:\n",
    "    X, y, _ = collect_representations(results, layer_idx)\n",
    "    if len(X) == 0:\n",
    "        continue\n",
    "    full_sim = sklearn_cosine(X)\n",
    "\n",
    "    within_sims, between_sims = [], []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(i + 1, len(X)):\n",
    "            if y[i] == y[j]:\n",
    "                within_sims.append(full_sim[i, j])\n",
    "            else:\n",
    "                between_sims.append(full_sim[i, j])\n",
    "\n",
    "    w = np.mean(within_sims) if within_sims else float(\"nan\")\n",
    "    b = np.mean(between_sims) if between_sims else float(\"nan\")\n",
    "    print(f\"{layer_idx:6d}  {w:10.4f}  {b:10.4f}  {w - b:10.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Model Direction Analysis\n",
    "\n",
    "Train a logistic regression classifier on self-ref vs. quoted and use its weight vector\n",
    "as a \"self-model direction.\"  Then project all three conditions onto this direction.\n",
    "\n",
    "If the model has a coherent notion of self-reference, roleplay representations should\n",
    "fall somewhere *between* self-ref and quoted speech on this axis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, len(analysis_layers), figsize=(5 * len(analysis_layers), 4))\n",
    "if len(analysis_layers) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for col, layer_idx in enumerate(analysis_layers):\n",
    "    X, y, _ = collect_representations(results, layer_idx)\n",
    "    if len(X) == 0:\n",
    "        continue\n",
    "\n",
    "    # Train self-ref (0) vs quoted (1) classifier\n",
    "    mask_bin = (y == 0) | (y == 1)\n",
    "    X_bin, y_bin = X[mask_bin], y[mask_bin]\n",
    "\n",
    "    if len(np.unique(y_bin)) < 2:\n",
    "        continue\n",
    "\n",
    "    clf = LogisticRegression(max_iter=2000, C=1.0, solver=\"lbfgs\")\n",
    "    clf.fit(X_bin, y_bin)\n",
    "\n",
    "    # Self-model direction = classifier weight vector (normalized)\n",
    "    direction = clf.coef_[0]\n",
    "    direction = direction / np.linalg.norm(direction)\n",
    "\n",
    "    # Project all samples\n",
    "    projections = X @ direction  # dot product\n",
    "\n",
    "    # Box plot by condition\n",
    "    ax = axes[col]\n",
    "    data_by_cond = []\n",
    "    tick_labels = []\n",
    "    colors = []\n",
    "    for cls_id, name in label_names.items():\n",
    "        mask = y == cls_id\n",
    "        if mask.sum() > 0:\n",
    "            data_by_cond.append(projections[mask])\n",
    "            tick_labels.append(name)\n",
    "            colors.append(color_map[name])\n",
    "\n",
    "    bp = ax.boxplot(data_by_cond, labels=tick_labels, patch_artist=True)\n",
    "    for patch, c in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(c)\n",
    "        patch.set_alpha(0.5)\n",
    "    ax.set_title(f\"Layer {layer_idx}\")\n",
    "    ax.set_ylabel(\"Self-model projection\")\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "fig.suptitle(\"Self-Model Direction: Projection of 'I' Representations\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"self_model_direction.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved self_model_direction.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-by-Layer Probe Accuracy\n",
    "\n",
    "Run the binary and 3-way linear probes at *every* layer to see where self-referential\n",
    "information emerges and peaks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_layers = list(range(N_LAYERS + 1))  # 0 = embedding, 1..N_LAYERS = transformer layers\n",
    "\n",
    "binary_accs = []\n",
    "binary_stds = []\n",
    "three_accs  = []\n",
    "three_stds  = []\n",
    "\n",
    "for layer_idx in all_layers:\n",
    "    X, y, _ = collect_representations(results, layer_idx)\n",
    "    if len(X) == 0:\n",
    "        binary_accs.append(float(\"nan\"))\n",
    "        binary_stds.append(0)\n",
    "        three_accs.append(float(\"nan\"))\n",
    "        three_stds.append(0)\n",
    "        continue\n",
    "\n",
    "    # Binary\n",
    "    mask_bin = (y == 0) | (y == 1)\n",
    "    X_bin, y_bin = X[mask_bin], y[mask_bin]\n",
    "    if len(np.unique(y_bin)) < 2 or len(y_bin) < 5:\n",
    "        binary_accs.append(float(\"nan\"))\n",
    "        binary_stds.append(0)\n",
    "    else:\n",
    "        clf = LogisticRegression(max_iter=2000, C=1.0, solver=\"lbfgs\")\n",
    "        k = min(5, min(np.bincount(y_bin)))\n",
    "        cv = StratifiedKFold(n_splits=max(k, 2), shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(clf, X_bin, y_bin, cv=cv, scoring=\"accuracy\")\n",
    "        binary_accs.append(scores.mean())\n",
    "        binary_stds.append(scores.std())\n",
    "\n",
    "    # 3-way\n",
    "    if len(np.unique(y)) < 3 or len(y) < 5:\n",
    "        three_accs.append(float(\"nan\"))\n",
    "        three_stds.append(0)\n",
    "    else:\n",
    "        clf3 = LogisticRegression(max_iter=2000, C=1.0, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "        k3 = min(5, min(np.bincount(y)))\n",
    "        cv3 = StratifiedKFold(n_splits=max(k3, 2), shuffle=True, random_state=42)\n",
    "        scores3 = cross_val_score(clf3, X, y, cv=cv3, scoring=\"accuracy\")\n",
    "        three_accs.append(scores3.mean())\n",
    "        three_stds.append(scores3.std())\n",
    "\n",
    "binary_accs = np.array(binary_accs)\n",
    "binary_stds = np.array(binary_stds)\n",
    "three_accs  = np.array(three_accs)\n",
    "three_stds  = np.array(three_stds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(all_layers, binary_accs, \"o-\", color=\"purple\", label=\"Binary (self-ref vs quoted)\", markersize=4)\n",
    "ax.fill_between(all_layers, binary_accs - binary_stds, binary_accs + binary_stds,\n",
    "                color=\"purple\", alpha=0.15)\n",
    "ax.plot(all_layers, three_accs, \"s-\", color=\"orange\", label=\"3-way\", markersize=4)\n",
    "ax.fill_between(all_layers, three_accs - three_stds, three_accs + three_stds,\n",
    "                color=\"orange\", alpha=0.15)\n",
    "ax.axhline(0.5, ls=\"--\", color=\"gray\", alpha=0.5, label=\"chance (binary)\")\n",
    "ax.axhline(1/3, ls=\":\", color=\"gray\", alpha=0.5, label=\"chance (3-way)\")\n",
    "ax.set_xlabel(\"Layer\")\n",
    "ax.set_ylabel(\"CV Accuracy\")\n",
    "ax.set_title(\"Linear Probe Accuracy by Layer\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"layer_probe_accuracy.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved layer_probe_accuracy.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Layers: {N_LAYERS}, Hidden dim: {model.config.hidden_size}\")\n",
    "\n",
    "print(f\"\\n--- Sample Counts ---\")\n",
    "for cond_name in results:\n",
    "    print(f\"  {cond_name}: {len(results[cond_name])} / {len(conditions[cond_name])} prompts yielded 'I' tokens\")\n",
    "\n",
    "print(f\"\\n--- Example Generated Texts ---\")\n",
    "for cond_name in texts:\n",
    "    print(f\"\\n  [{cond_name}]\")\n",
    "    for t in texts[cond_name][:3]:\n",
    "        print(f\"    {t[:100]}\")\n",
    "\n",
    "print(f\"\\n--- Probe Results (selected layers) ---\")\n",
    "print(f\"{'Layer':>6s}  {'Binary':>8s}  {'3-way':>8s}\")\n",
    "for layer_idx, res in sorted(probe_results.items()):\n",
    "    print(f\"{layer_idx:6d}  {res['binary']:8.3f}  {res['three_way']:8.3f}\")\n",
    "\n",
    "# Best layers\n",
    "best_bin_layer = max(probe_results, key=lambda l: probe_results[l][\"binary\"])\n",
    "best_3_layer   = max(probe_results, key=lambda l: probe_results[l][\"three_way\"])\n",
    "print(f\"\\nBest binary probe: layer {best_bin_layer} ({probe_results[best_bin_layer]['binary']:.3f})\")\n",
    "print(f\"Best 3-way probe:  layer {best_3_layer} ({probe_results[best_3_layer]['three_way']:.3f})\")\n",
    "print(\"\\nDone.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}